# Train
epochs: 800
blr: 0.001 # this is base_lr = 1e-4, lr = base_lr * batch_size / 256
warmup_lr: 0.000001 # 1e-6
min_lr: 0.
warmup_epochs: 20
batch_size: 300 # 136 # 256 <---------------------
save_ckpt_freq: 3

# Data
data_path: '/kaggle/input/multimae-v1' # Change me
data_paths:
  - 'datasets/multimae-v1'
  - 'datasets/nyu-depth-v2'

# Wandb logging
log_wandb: True # Set to True to log to Weights & Biases
wandb_project: PretrainSMultiMAE
output_dir: './output/pretrain' # Change directory if needed
normalized_depth: True

depth_range: 256
depth_loss: "l1"
pretrained_weights: 'pretrained_weights/mae-b_dec512d8b_1600e_multivit-c477195b.pth'
pretrained_backbone: 'mae'
lr_scale: 0.01
data_augmentation_version: 2
gpus:
  - 3
